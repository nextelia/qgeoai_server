"""
YOLO HTML Report Generator for QModel Trainer

Specialized report generator for YOLO11 training (detection, segmentation, OBB).
Uses plots and metrics generated by Ultralytics during training.
"""

import csv
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

from .templates import get_html_template


class YOLOHTMLReportGenerator:
    """Generator for YOLO training reports."""
    
    def __init__(self, output_dir: str, task: str = 'yolo_detection'):
        """
        Initialize YOLO report generator.
        
        Args:
            output_dir: Training output directory (contains train/ subdirectory)
            task: YOLO task ('yolo_detection', 'yolo_segmentation', 'yolo_obb')
        """
        self.output_dir = Path(output_dir)
        self.task = task
        
        # YOLO outputs are in train/ subdirectory
        self.train_dir = self.output_dir / 'train'
        
        # Create report directory
        self.report_dir = self.output_dir / 'report'
        self.report_dir.mkdir(parents=True, exist_ok=True)
    
    def generate(
        self,
        config: Dict[str, Any],
        history: Dict[str, List[float]],
        trainer: Any
    ) -> str:
        """
        Generate complete HTML report for YOLO training.
        
        Args:
            config: Training configuration
            history: Training history from trainer
            trainer: YOLO trainer instance
            
        Returns:
            Path to generated report
        """
        # Get best metric info
        best_metric = trainer.best_metric
        best_epoch = trainer.best_epoch
        
        # Parse results.csv for additional metrics
        results_data = self._parse_results_csv()
        
        # Generate qualitative examples (validation batch files)
        val_batches = self._find_validation_batches()
        # DEBUG: Log found batches
        print(f"DEBUG: Found validation batches: {val_batches}")
        for batch_num in val_batches:
            labels_path = self.train_dir / f'val_batch{batch_num}_labels.jpg'
            pred_path = self.train_dir / f'val_batch{batch_num}_pred.jpg'
            print(f"  Labels exist: {labels_path.exists()} - {labels_path}")
            print(f"  Pred exist: {pred_path.exists()} - {pred_path}")
        
        # Get template
        html_content = get_html_template()
        
        # Fill all sections
        html_content = self._fill_summary_section(html_content, config, history, best_metric, best_epoch)
        html_content = self._fill_dataset_section(html_content, config)
        html_content = html_content.replace('{{LR_FINDER_SECTION}}', '')  # No LR finder for YOLO
        html_content = self._fill_training_curves(html_content)
        html_content = self._fill_qualitative_section(html_content, val_batches)
        html_content = self._fill_confusion_matrix_section(html_content)
        html_content = self._fill_metrics_section(html_content, results_data, best_epoch)
        html_content = self._fill_technical_info(html_content, config)
        html_content = self._fill_metadata(html_content, config)

         # Remove PyTorch-specific JavaScript that hides YOLO examples
        html_content = html_content.replace(
            "switchExamples('medium');",
            "// switchExamples('medium'); // Disabled for YOLO"
        )
        
        # Save report
        report_path = self.report_dir / 'training_report.html'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return str(report_path)
    
    def _parse_results_csv(self) -> Dict[str, List[float]]:
        """Parse YOLO results.csv file."""
        csv_path = self.train_dir / 'results.csv'
        
        if not csv_path.exists():
            return {}
        
        results = {}
        
        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            
            # Initialize lists for each column
            for column in reader.fieldnames:
                results[column] = []
            
            # Read all rows
            for row in reader:
                for column, value in row.items():
                    try:
                        results[column].append(float(value.strip()))
                    except (ValueError, AttributeError):
                        results[column].append(0.0)
        
        return results
    
    def _find_validation_batches(self) -> List[int]:
        """Find all available validation batch files."""
        pred_files = list(self.train_dir.glob('val_batch*_pred.jpg'))
        
        batch_numbers = []
        for pred_file in pred_files:
            try:
                # Extract batch number from filename: val_batch0_pred.jpg -> 0
                batch_num = int(pred_file.stem.split('val_batch')[1].split('_pred')[0])
                
                # Check if corresponding labels file exists
                labels_file = self.train_dir / f'val_batch{batch_num}_labels.jpg'
                if labels_file.exists():
                    batch_numbers.append(batch_num)
            except (IndexError, ValueError):
                continue
        
        # Sort and limit to 4 examples
        batch_numbers.sort()
        return batch_numbers[:4]
    
    def _fill_summary_section(
        self,
        html: str,
        config: Dict[str, Any],
        history: Dict[str, List[float]],
        best_metric: float,
        best_epoch: int
    ) -> str:
        """Fill executive summary section."""
        model_name = config.get('model_name', 'YOLO11 Model')
        
        # Architecture string
        model_size = config.get('model_size', 'n')
        task_name = self._get_task_display_name()
        architecture = f"YOLO11{model_size}"
        
        # Dataset info
        total_images = config.get('total_images', 'N/A')
        dataset_size_str = str(total_images) if total_images != 'N/A' else 'N/A'
        
        total_epochs = len(history['train_loss'])
        
        # Training time
        if 'total_train_time' in config:
            total_seconds = config['total_train_time']
            hours = int(total_seconds // 3600)
            minutes = int((total_seconds % 3600) // 60)
            seconds = int(total_seconds % 60)
            
            if hours > 0:
                train_time_str = f"{hours}h {minutes}m {seconds}s"
            elif minutes > 0:
                train_time_str = f"{minutes}m {seconds}s"
            else:
                train_time_str = f"{seconds}s"
        else:
            train_time_str = f"~{total_epochs} min (estimated)"
        
        # Quality assessment based on mAP
        quality_class, quality_text = self._get_quality_assessment(best_metric)
        
        # Summary text
        metric_name = "mAP50-95"
        summary_text = (
            f"The {architecture} model was trained for {total_epochs} epochs on {dataset_size_str} images. "
            f"Best validation {metric_name} of <strong>{best_metric:.4f}</strong> was achieved at epoch {best_epoch + 1}. "
            f"{quality_text}"
        )
        
        html = html.replace('{{MODEL_NAME}}', model_name)
        html = html.replace('{{ARCHITECTURE}}', architecture)
        html = html.replace('{{DATASET_SIZE}}', dataset_size_str)
        html = html.replace('{{EPOCHS}}', str(total_epochs))
        html = html.replace('{{LEARNING_RATE}}', f"{config.get('learning_rate', 0.001):.2e}")
        html = html.replace('{{BEST_EPOCH}}', str(best_epoch + 1))
        html = html.replace('{{TRAIN_TIME}}', train_time_str)
        html = html.replace('{{SUMMARY_TEXT}}', summary_text)
        html = html.replace('{{BEST_METRIC}}', f"{best_metric:.4f}")
        html = html.replace('{{BEST_METRIC_LABEL}}', 'Best mAP50-95')
        
        return html
    
    def _fill_dataset_section(
        self,
        html: str,
        config: Dict[str, Any]
    ) -> str:
        """Fill dataset details section."""
        class_names = config.get('class_names', [])

        if class_names:
            table_html = '<table><thead><tr><th>ID</th><th>Class Name</th></tr></thead><tbody>'
            
            # Handle both list and dict formats
            if isinstance(class_names, dict):
                for class_id, class_name in class_names.items():
                    table_html += f'<tr><td><strong>{class_id}</strong></td><td>{class_name}</td></tr>'
            else:
                for idx, class_name in enumerate(class_names):
                    table_html += f'<tr><td><strong>{idx}</strong></td><td>{class_name}</td></tr>'
            
            table_html += '</tbody></table>'
            
            # No class distribution info for YOLO
            table_html += '''
                <div style="margin-top:10px;padding:10px;background:#f0f9ff;color:#075985;border-radius:8px;font-size:13px;">
                    ‚ÑπÔ∏è <strong>Note:</strong> Class distribution statistics are computed by Ultralytics during training.
                </div>
            '''
        else:
            table_html = '<p class="small" style="color:#999;">No class information available</p>'
        
        html = html.replace('{{DATASET_TABLE}}', table_html)
        
        # Training config
        tile_size = config.get('image_size', 512)
        html = html.replace('{{TILE_SIZE}}', f"{tile_size}√ó{tile_size}")
        
        # Val split info
        num_train = config.get('num_train_images', 'N/A')
        num_val = config.get('num_val_images', 'N/A')
        if num_train != 'N/A' and num_val != 'N/A':
            val_split_str = f"{num_train} train / {num_val} val"
        else:
            val_split_str = "Split done at export"
        html = html.replace('{{VAL_SPLIT}}', val_split_str)
        
        batch_size = config.get('batch_size', 'N/A')
        html = html.replace('{{BATCH_SIZE}}', str(batch_size))
        
        return html
    
    def _fill_training_curves(self, html: str) -> str:
        """Fill training curves section using YOLO's results.png."""
        results_png = self.train_dir / 'results.png'
        
        if results_png.exists():
            curves_section = '''
            <div class="card" style="margin-top: 16px">
                <div class="section-title">Training Curves</div>
                <div class="explain" style="margin-bottom:10px;">
                Comprehensive training overview including loss functions, performance metrics, mAP evolution curves, and learning rate schedule.
                <strong>Click to enlarge.</strong>
                </div>
                <div class="chart">
                <img src="../train/results.png" alt="Training curves" onclick="openLightbox(this)" style="width:100%;" />
                </div>
                <div class="click-hint">üîç Click to enlarge</div>
            </div>'''
        else:
            curves_section = '<div class="card" style="margin-top: 16px"><p>Training curves not available.</p></div>'
        
        html = html.replace('{{TRAINING_CURVES_SECTION}}', curves_section)
        return html
    
    def _fill_qualitative_section(
        self,
        html: str,
        val_batches: List[int]
    ) -> str:
        """Fill qualitative examples section using YOLO validation batches."""
        if not val_batches:
            section_html = '''
          <div class="card" style="margin-top: 16px">
            <div class="section-title">Validation Examples</div>
            <div style="padding: 20px; background: #fef3c7; border-radius: 8px; text-align: center;">
                <strong>‚ö†Ô∏è No validation examples found</strong>
                <p style="margin-top: 8px; color: #92400e;">Validation batch images were not generated during training.</p>
            </div>
          </div>'''
        else:
            section_html = '''
          <div class="card" style="margin-top: 16px">
            <div class="section-title">Validation Examples</div>
            <div class="explain" style="margin-bottom:10px;">
              Visual comparison of predictions vs ground truth on validation batches.
              Images show bounding boxes, masks, or oriented boxes depending on task.
            </div>
            
            <div class="examples-grid">'''
            
            for batch_num in val_batches:
                section_html += f'''
              <div class="example-pair">
                <div class="pair-header">Validation Batch {batch_num}</div>
                <div class="pair-row">
                  <div class="pair-col">
                    <div class="pair-label">Ground Truth</div>
                    <img src="../train/val_batch{batch_num}_labels.jpg" alt="Labels" onclick="openLightbox(this)" />
                  </div>
                  <div class="pair-col">
                    <div class="pair-label">Predictions</div>
                    <img src="../train/val_batch{batch_num}_pred.jpg" alt="Predictions" onclick="openLightbox(this)" />
                  </div>
                </div>
              </div>'''
            
            section_html += '''
            </div>
          </div>'''
        
        html = html.replace('{{QUALITATIVE_EXAMPLES_SECTION}}', section_html)
        
        return html
    
    def _fill_confusion_matrix_section(self, html: str) -> str:
        """Disable confusion matrix for YOLO (known accuracy issues)."""
        section_html = '''
        <div class="card" style="margin-top: 16px">
            <div class="section-title">Confusion Matrix</div>
            <div style="padding: 20px; background: #fef3c7; border-radius: 8px;">
            <div style="display: flex; align-items: start; gap: 12px;">
                <div style="font-size: 24px;">‚ö†Ô∏è</div>
                <div>
                <strong style="color: #92400e;">Confusion Matrix Disabled</strong>
                <p style="margin-top: 8px; color: #92400e; line-height: 1.5;">
                    The confusion matrix generated by Ultralytics YOLO has known accuracy issues for detection and segmentation tasks.
                    To maintain scientific integrity, this metric has been disabled.
                    Please rely on <strong>mAP, Precision, and Recall</strong> metrics which are computed correctly.
                </p>
                </div>
            </div>
            </div>
        </div>'''
        
        html = html.replace('{{CONFUSION_MATRIX_SECTION}}', section_html)
        
        return html
    
    def _fill_metrics_section(
        self,
        html: str,
        results_data: Dict[str, List[float]],
        best_epoch: int
    ) -> str:
        """Fill validation metrics table from results.csv."""
        if not results_data or best_epoch >= len(results_data.get('epoch', [])):
            # Fallback to empty table
            metrics_html = '<tr><td colspan="3">No metrics available</td></tr>'
            html = html.replace('{{METRICS_TABLE}}', metrics_html)
            return html
        
        metrics_html = ''
        
        # Define metrics to display based on task
        if self.task == 'yolo_segmentation':
            metrics_to_show = [
                ('metrics/mAP50-95(M)', 'mAP50-95 (Mask)'),
                ('metrics/mAP50(M)', 'mAP50 (Mask)'),
                ('metrics/precision(M)', 'Precision (Mask)'),
                ('metrics/recall(M)', 'Recall (Mask)'),
            ]
        else:
            # Detection and OBB
            metrics_to_show = [
                ('metrics/mAP50-95(B)', 'mAP50-95'),
                ('metrics/mAP50(B)', 'mAP50'),
                ('metrics/precision(B)', 'Precision'),
                ('metrics/recall(B)', 'Recall'),
            ]
        
        for metric_key, metric_label in metrics_to_show:
            if metric_key in results_data:
                values = results_data[metric_key]
                
                if best_epoch < len(values):
                    value_at_best = values[best_epoch]
                    peak_val = max(values)
                    peak_epoch = values.index(peak_val)
                    
                    if peak_epoch != best_epoch:
                        note = f'<span style="color:#999;font-size:11px;">(peaked at {peak_val:.4f} on epoch {peak_epoch + 1})</span>'
                    else:
                        note = '<span style="color:#16a34a;font-size:11px;">‚úì Peak value</span>'
                    
                    metrics_html += f'''
                        <tr>
                          <td><strong>{metric_label}</strong></td>
                          <td style="font-size:16px;font-weight:bold;">{value_at_best:.4f}</td>
                          <td>{note}</td>
                        </tr>'''
        
        if not metrics_html:
            metrics_html = '<tr><td colspan="3">No metrics available</td></tr>'
        
        html = html.replace('{{METRICS_TABLE}}', metrics_html)
        
        return html
    
    def _fill_technical_info(
        self,
        html: str,
        config: Dict[str, Any]
    ) -> str:
        """Fill technical information section."""
        tech_html = ''
        
        # Device info
        device = config.get('device', 'cuda')
        if device.lower() == 'cuda':
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                tech_html += f'<div class="kv"><b>{gpu_name}</b><span class="small">GPU</span></div>'
            else:
                tech_html += '<div class="kv"><b>CUDA</b><span class="small">Device</span></div>'
        else:
            tech_html += f'<div class="kv"><b>{device.upper()}</b><span class="small">Device</span></div>'
        
        # Ultralytics version
        try:
            from ultralytics import __version__ as ultralytics_version
            tech_html += f'<div class="kv"><b>{ultralytics_version}</b><span class="small">Ultralytics</span></div>'
        except:
            tech_html += '<div class="kv"><b>YOLO11</b><span class="small">Framework</span></div>'
        
        # Model size
        model_size = config.get('model_size', 'n')
        size_map = {'n': 'Nano', 's': 'Small', 'm': 'Medium', 'l': 'Large', 'x': 'XLarge'}
        size_label = size_map.get(model_size, model_size)
        tech_html += f'<div class="kv"><b>{size_label}</b><span class="small">Model Size</span></div>'
        
        html = html.replace('{{TECHNICAL_INFO}}', tech_html)
        
        # Training config
        optimizer_name = config.get('optimizer_name', 'SGD')
        html = html.replace('{{OPTIMIZER}}', optimizer_name)
        
        html = html.replace('{{SCHEDULER}}', 'Managed by Ultralytics')
        
        patience = config.get('patience', 'None')
        es_text = f"Yes (patience={patience})" if patience else "No"
        html = html.replace('{{EARLY_STOPPING}}', es_text)
        
        pretrained = "Yes (COCO)" if config.get('pretrained', False) else "No"
        html = html.replace('{{PRETRAINED}}', pretrained)
        
        return html
    
    def _fill_metadata(self, html: str, config: Dict[str, Any]) -> str:
        """Fill metadata."""
        date_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        plugin_version = config.get('plugin_version', '1.0.0')
        
        html = html.replace('{{DATE}}', date_str)
        html = html.replace('{{PLUGIN_VERSION}}', plugin_version)
        
        return html
    
    def _get_task_display_name(self) -> str:
        """Get display name for task."""
        task_map = {
            'yolo_detection': 'Object Detection',
            'yolo_segmentation': 'Instance Segmentation',
            'yolo_obb': 'Oriented Bounding Boxes'
        }
        return task_map.get(self.task, self.task)
    
    def _get_quality_assessment(self, best_metric: float) -> tuple:
        """Get quality assessment based on mAP."""
        if best_metric >= 0.80:
            return ('excellent', 'Excellent performance.')
        elif best_metric >= 0.60:
            return ('good', 'Good performance, consider fine-tuning for improvement.')
        elif best_metric >= 0.40:
            return ('moderate', 'Moderate performance, additional training or data may help.')
        else:
            return ('low', 'Low performance, review dataset quality and model configuration.')